<!doctype html>
<html lang="fr">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Jarvis — Mode Voiture</title>
  <style>
    :root { font-family: system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif; }
    body { margin: 0; padding: 16px; background:#0b0f14; color:#e8eef6; }
    h1 { margin: 0 0 12px; font-size: 22px; }
    .card { background:#121a24; border:1px solid #1f2a3a; border-radius:14px; padding:14px; max-width: 900px; margin:0 auto; }
    .row { display:flex; gap:10px; flex-wrap:wrap; align-items:center; }
    .btn {
      border:1px solid #2a3a52; background:#182235; color:#e8eef6;
      padding:14px 16px; border-radius:12px; cursor:pointer; font-weight:600;
      min-width: 140px;
    }
    .btn:disabled { opacity:.5; cursor:not-allowed; }
    .btnPrimary { background:#2a5bd7; border-color:#2a5bd7; }
    .btnDanger { background:#b42318; border-color:#b42318; }
    .pill { display:inline-flex; align-items:center; gap:8px; padding:8px 10px; border:1px solid #26344a; border-radius:999px; background:#0f1622; }
    .dot { width:10px; height:10px; border-radius:50%; background:#888; }
    .dot.on { background:#33d17a; }
    .dot.speaking { background:#f5c211; }
    .dot.sending { background:#62a0ea; }
    label { display:block; margin:14px 0 6px; font-size:13px; opacity:.9; }
    input, textarea {
      width:100%; box-sizing:border-box; padding:12px; border-radius:12px;
      border:1px solid #233148; background:#0f1622; color:#e8eef6;
      font-size:15px;
    }
    textarea { min-height: 92px; resize: vertical; }
    .out { min-height: 110px; font-size:16px; line-height:1.35; }
    details { margin-top: 10px; }
    summary { cursor:pointer; opacity:.9; }
    .hint { font-size: 12px; opacity:.75; margin-top: 8px; }
    .big { font-size: 18px; }
  </style>
</head>

<body>
  <div class="card">
    <h1>Jarvis — Mode Voiture → n8n</h1>

    <div class="row" style="margin-bottom:10px;">
      <button id="btnStart" class="btn btnPrimary">Démarrer</button>
      <button id="btnStop" class="btn btnDanger" disabled>Stop</button>
      <button id="btnSpeak" class="btn" disabled>Relire</button>
      <button id="btnClear" class="btn">Effacer</button>

      <span class="pill">
        <span id="dot" class="dot"></span>
        <span id="status">prêt</span>
      </span>
    </div>

    <label>Webhook n8n (prod)</label>
    <input id="webhook" type="url" value="https://n8n.srv765349.hstgr.cloud/webhook/jarvis" />

    <label>Texte (reconnu / saisi)</label>
    <textarea id="inputText" placeholder="Parle ou tape ici…"></textarea>

    <label>Réponse (propre)</label>
    <textarea id="outputText" class="out" readonly placeholder="La réponse apparaîtra ici…"></textarea>

    <details>
      <summary>Debug (réponse brute)</summary>
      <textarea id="debugText" readonly></textarea>
    </details>

    <div class="hint">
      Astuce : sur GitHub Pages (HTTPS), Chrome retient mieux l’autorisation micro.
      Il faut quand même une première action utilisateur (bouton “Démarrer”).
    </div>
  </div>

<script>
(() => {
  // ----------------------------
  // CONFIG
  // ----------------------------
  const SILENCE_MS = 850;     // délai “fin de phrase” (mode voiture)
  const RESTART_DELAY_MS = 350; // petite pause avant de relancer l'écoute
  const MIN_CHARS_TO_SEND = 2;  // évite d'envoyer du vide/bruit

  // ----------------------------
  // UI refs
  // ----------------------------
  const btnStart = document.getElementById('btnStart');
  const btnStop  = document.getElementById('btnStop');
  const btnSpeak = document.getElementById('btnSpeak');
  const btnClear = document.getElementById('btnClear');

  const dot = document.getElementById('dot');
  const statusEl = document.getElementById('status');

  const webhookEl = document.getElementById('webhook');
  const inputEl   = document.getElementById('inputText');
  const outputEl  = document.getElementById('outputText');
  const debugEl   = document.getElementById('debugText');

  // Save webhook URL (so it stays even after refresh)
  const LS_KEY = "jarvis_webhook_url_v1";
  const saved = localStorage.getItem(LS_KEY);
  if (saved) webhookEl.value = saved;
  webhookEl.addEventListener('change', () => localStorage.setItem(LS_KEY, webhookEl.value.trim()));

  // ----------------------------
  // SpeechRecognition setup
  // ----------------------------
  const SR = window.SpeechRecognition || window.webkitSpeechRecognition;
  const hasSR = !!SR;

  let recognition = null;

  // state
  let running = false;
  let isListening = false;
  let isSpeaking = false;
  let isSending = false;

  let finalTranscript = "";
  let silenceTimer = null;

  function setStatus(text, mode) {
    statusEl.textContent = text;
    dot.classList.remove('on', 'speaking');
    if (mode === 'listening') dot.classList.add('on');
    if (mode === 'speaking') dot.classList.add('speaking');
    if (mode === 'sending') dot.classList.add('speaking'); // reuse yellow-ish vibe
  }

  function enableUI(on) {
    btnStart.disabled = on;
    btnStop.disabled = !on;
    btnSpeak.disabled = !outputEl.value.trim();
  }

  function safeStopRecognition() {
    if (!recognition) return;
    try { recognition.onend = null; recognition.onerror = null; recognition.onresult = null; } catch(e) {}
    try { recognition.stop(); } catch(e) {}
  }

  function startListening() {
    if (!recognition || !running) return;
    if (isSpeaking || isSending) return;

    try {
      recognition.start();
      isListening = true;
      setStatus("écoute…", "listening");
    } catch (e) {
      // start() throws if already started; ignore
    }
  }

  function stopListening() {
    if (!recognition) return;
    try { recognition.stop(); } catch(e) {}
    isListening = false;
  }

  function resetSilenceTimer() {
    if (silenceTimer) clearTimeout(silenceTimer);
    silenceTimer = setTimeout(() => {
      // Fin de phrase détectée
      if (!running) return;
      if (isSpeaking || isSending) return;

      const text = inputEl.value.trim();
      if (text.length >= MIN_CHARS_TO_SEND) {
        sendToWebhook(text);
      } else {
        // rien à envoyer, on relance l'écoute
        restartListeningSoon();
      }
    }, SILENCE_MS);
  }

  function restartListeningSoon() {
    if (!running) return;
    setTimeout(() => {
      if (!running) return;
      if (isSpeaking || isSending) return;
      startListening();
    }, RESTART_DELAY_MS);
  }

  async function sendToWebhook(text) {
    if (isSending) return;
    isSending = true;
    setStatus("envoi…", "sending");

    // Stop micro pendant l'appel + pendant le TTS
    stopListening();

    const url = webhookEl.value.trim();
    if (!url) {
      isSending = false;
      setStatus("Webhook manquant", "");
      restartListeningSoon();
      return;
    }

    // payload simple
    const payload = { text };

    let raw = "";
    let parsed = null;
    try {
      const res = await fetch(url, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify(payload),
      });

      raw = await res.text();
      debugEl.value = raw;

      // Tentative parse JSON
      try {
        parsed = JSON.parse(raw);
      } catch (_) {
        parsed = null;
      }

      const clean = extractCleanOutput(parsed, raw);
      outputEl.value = clean;
      btnSpeak.disabled = !clean.trim();

      // Lire à voix haute et relancer l'écoute à la fin
      await speak(clean);

    } catch (err) {
      debugEl.value = String(err);
      outputEl.value = "Erreur : impossible de contacter n8n.";
      btnSpeak.disabled = false;
      // On évite de parler l'erreur si tu préfères : mais on peut.
      await speak(outputEl.value);
    } finally {
      isSending = false;
      if (running) restartListeningSoon();
    }
  }

  function extractCleanOutput(parsed, rawText) {
    // Cas #1 : n8n renvoie { "output": "..." }
    if (parsed && typeof parsed === "object") {
      // parfois c'est un item n8n: { output: "...", ... }
      if (typeof parsed.output === "string") return parsed.output;

      // parfois enveloppé : { data: { output: "..." } } etc.
      const deep = findStringByKey(parsed, "output");
      if (deep) return deep;

      // si c'est un tableau d'items
      if (Array.isArray(parsed)) {
        for (const it of parsed) {
          if (it && typeof it.output === "string") return it.output;
          const d = findStringByKey(it, "output");
          if (d) return d;
        }
      }

      // fallback : stringify court
      return JSON.stringify(parsed);
    }

    // Cas #2 : texte brut
    return String(rawText ?? "").trim();
  }

  function findStringByKey(obj, key) {
    // petit DFS safe
    const seen = new Set();
    const stack = [obj];
    while (stack.length) {
      const cur = stack.pop();
      if (!cur || typeof cur !== "object") continue;
      if (seen.has(cur)) continue;
      seen.add(cur);

      if (Object.prototype.hasOwnProperty.call(cur, key) && typeof cur[key] === "string") {
        return cur[key];
      }
      for (const k of Object.keys(cur)) {
        const v = cur[k];
        if (v && typeof v === "object") stack.push(v);
      }
    }
    return "";
  }

  function speak(text) {
    return new Promise((resolve) => {
      const t = (text || "").trim();
      if (!t) return resolve();

      if (!("speechSynthesis" in window)) return resolve();

      // STOP micro pendant que ça parle (anti-boucle)
      stopListening();
      isSpeaking = true;
      setStatus("parle…", "speaking");

      // Nettoyage file d'attente
      try { window.speechSynthesis.cancel(); } catch(e) {}

      const utter = new SpeechSynthesisUtterance(t);
      utter.lang = "fr-FR";
      utter.rate = 1.0;
      utter.pitch = 1.0;

      utter.onend = () => {
        isSpeaking = false;
        setStatus("ok ✅", "");
        resolve();
      };
      utter.onerror = () => {
        isSpeaking = false;
        setStatus("ok ✅", "");
        resolve();
      };

      window.speechSynthesis.speak(utter);
    });
  }

  // ----------------------------
  // Init recognition
  // ----------------------------
  function init() {
    if (!hasSR) {
      setStatus("SpeechRecognition non supporté (Chrome conseillé)", "");
      btnStart.disabled = true;
      return;
    }

    recognition = new SR();
    recognition.lang = "fr-FR";
    recognition.interimResults = true;
    recognition.continuous = true; // on gère nous-mêmes la “fin de phrase” via silence timer

    recognition.onresult = (event) => {
      if (!running) return;
      if (isSpeaking || isSending) return; // anti-boucle

      let interim = "";
      for (let i = event.resultIndex; i < event.results.length; i++) {
        const res = event.results[i];
        const txt = res[0]?.transcript || "";
        if (res.isFinal) {
          finalTranscript += txt;
        } else {
          interim += txt;
        }
      }

      const full = (finalTranscript + " " + interim).replace(/\s+/g, " ").trim();
      inputEl.value = full;

      // Reset timer à chaque “activité”
      resetSilenceTimer();
    };

    recognition.onerror = (e) => {
      // "not-allowed" si permission refusée, "no-speech" si rien capté, etc.
      if (!running) return;
      debugEl.value = "SpeechRecognition error: " + (e?.error || JSON.stringify(e));

      // Si pas de speech, on relance
      if (e?.error === "no-speech" || e?.error === "aborted") {
        restartListeningSoon();
      } else {
        setStatus("Erreur micro / autorisation", "");
      }
    };

    recognition.onend = () => {
      isListening = false;
      if (!running) return;

      // on relance si on est censé tourner
      if (!isSpeaking && !isSending) {
        restartListeningSoon();
      }
    };

    setStatus("prêt", "");
  }

  // ----------------------------
  // UI actions
  // ----------------------------
  btnStart.addEventListener('click', async () => {
    if (!recognition) init();
    running = true;
    finalTranscript = "";
    enableUI(true);
    setStatus("démarré ✅", "");
    // Important: premier start = geste utilisateur -> permission micro
    startListening();
  });

  btnStop.addEventListener('click', () => {
    running = false;
    isListening = false;
    isSending = false;
    isSpeaking = false;
    if (silenceTimer) clearTimeout(silenceTimer);
    try { window.speechSynthesis.cancel(); } catch(e) {}
    stopListening();
    enableUI(false);
    setStatus("arrêté", "");
  });

  btnSpeak.addEventListener('click', async () => {
    const t = outputEl.value.trim();
    if (!t) return;
    await speak(t);
    if (running) restartListeningSoon();
  });

  btnClear.addEventListener('click', () => {
    finalTranscript = "";
    inputEl.value = "";
    outputEl.value = "";
    debugEl.value = "";
    btnSpeak.disabled = true;
    if (running) setStatus("écoute…", "listening");
  });

  // si l’utilisateur tape au clavier, on peut envoyer avec Entrée (optionnel)
  inputEl.addEventListener('keydown', (e) => {
    if (e.key === "Enter" && (e.ctrlKey || e.metaKey)) {
      e.preventDefault();
      const text = inputEl.value.trim();
      if (text.length >= MIN_CHARS_TO_SEND) sendToWebhook(text);
    }
  });

  init();
})();
</script>
</body>
</html>
